/**
 * Service d'embeddings avec Sentence Transformers
 * Origine : Bas√© sur les mod√®les BERT/RoBERTa (2018-2019)
 * Fonctionnement : Encode le texte en vecteurs de 384 dimensions
 */

const { pipeline } = require('@xenova/transformers');
const natural = require('natural');
const fs = require('fs').promises;
const path = require('path');

class EmbeddingService {
    constructor() {
        this.model = null;
        this.modelName = 'Xenova/all-MiniLM-L6-v2'; // Mod√®le optimis√© pour la similarit√© s√©mantique
        this.cache = new Map();
        this.cacheFile = path.join(__dirname, '../data/embeddings_cache.json');
        this.isInitialized = false;
    }

    /**
     * Initialisation du mod√®le Sentence Transformers
     * Le mod√®le all-MiniLM-L6-v2 :
     * - Bas√© sur BERT (Bidirectional Encoder Representations from Transformers)
     * - Optimis√© pour la vitesse tout en gardant une bonne pr√©cision
     * - Produit des embeddings de 384 dimensions
     * - Entra√Æn√© sur des paires de phrases pour la similarit√© s√©mantique
     */
    async initialize() {
        if (this.isInitialized) return;

        try {
            console.log('üîÑ Initialisation du mod√®le Sentence Transformers...');
            
            // Chargement du mod√®le de feature extraction
            this.model = await pipeline('feature-extraction', this.modelName, {
                quantized: false, // Pour une meilleure pr√©cision
                revision: 'main'
            });

            // Chargement du cache existant
            await this.loadCache();

            this.isInitialized = true;
            console.log('‚úÖ Mod√®le Sentence Transformers initialis√©');
        } catch (error) {
            console.error('‚ùå Erreur initialisation mod√®le:', error);
            throw new Error('Impossible d\'initialiser le service d\'embeddings');
        }
    }

    /**
     * Pr√©processing du texte avant embedding
     * Origine : Techniques NLP classiques + optimisations pour Transformers
     */
    preprocessText(text) {
        if (!text || typeof text !== 'string') return '';

        return text
            .toLowerCase() // Normalisation de la casse
            .trim() // Suppression des espaces
            .replace(/[^\w\s√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]/g, ' ') // Nettoyage caract√®res sp√©ciaux
            .replace(/\s+/g, ' ') // Normalisation des espaces
            .substring(0, 512); // Limitation pour le mod√®le (max 512 tokens)
    }

    /**
     * G√©n√©ration d'embedding pour un texte
     * Utilise le mod√®le Sentence Transformers pour encoder le texte
     */
    async getEmbedding(text) {
        if (!this.isInitialized) {
            await this.initialize();
        }

        const cleanText = this.preprocessText(text);
        if (!cleanText) return null;

        // V√©rification du cache
        const cacheKey = this.generateCacheKey(cleanText);
        if (this.cache.has(cacheKey)) {
            return this.cache.get(cacheKey);
        }

        try {
            // G√©n√©ration de l'embedding avec le mod√®le
            const output = await this.model(cleanText, {
                pooling: 'mean', // Mean pooling pour obtenir un vecteur par phrase
                normalize: true   // Normalisation L2 pour la similarit√© cosinus
            });

            // Extraction du vecteur (premi√®re dimension)
            const embedding = Array.from(output.data);

            // Mise en cache
            this.cache.set(cacheKey, embedding);
            
            return embedding;
        } catch (error) {
            console.error('Erreur g√©n√©ration embedding:', error);
            return null;
        }
    }

    /**
     * G√©n√©ration d'embeddings pour plusieurs textes (batch)
     * Plus efficace que les appels individuels
     */
    async getBatchEmbeddings(texts) {
        if (!Array.isArray(texts)) return [];

        const embeddings = [];
        
        // Traitement par batch de 10 pour optimiser la m√©moire
        const batchSize = 10;
        for (let i = 0; i < texts.length; i += batchSize) {
            const batch = texts.slice(i, i + batchSize);
            const batchPromises = batch.map(text => this.getEmbedding(text));
            const batchResults = await Promise.all(batchPromises);
            embeddings.push(...batchResults);
        }

        return embeddings;
    }

    /**
     * Calcul de similarit√© cosinus entre deux vecteurs
     * Origine : G√©om√©trie vectorielle, optimis√© pour les embeddings normalis√©s
     * 
     * Formule : cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)
     * Avec des vecteurs normalis√©s : cos(Œ∏) = A ¬∑ B
     */
    cosineSimilarity(vectorA, vectorB) {
        if (!vectorA || !vectorB || vectorA.length !== vectorB.length) {
            return 0;
        }

        let dotProduct = 0;
        let normA = 0;
        let normB = 0;

        for (let i = 0; i < vectorA.length; i++) {
            dotProduct += vectorA[i] * vectorB[i];
            normA += vectorA[i] * vectorA[i];
            normB += vectorB[i] * vectorB[i];
        }

        // √âviter la division par z√©ro
        if (normA === 0 || normB === 0) return 0;

        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    /**
     * Recherche des textes les plus similaires
     * Utilise la similarit√© cosinus pour classer les r√©sultats
     */
    async findMostSimilar(queryText, candidateTexts, topK = 5) {
        const queryEmbedding = await this.getEmbedding(queryText);
        if (!queryEmbedding) return [];

        const candidateEmbeddings = await this.getBatchEmbeddings(candidateTexts);
        
        const similarities = candidateEmbeddings.map((embedding, index) => ({
            text: candidateTexts[index],
            similarity: this.cosineSimilarity(queryEmbedding, embedding),
            index
        }));

        // Tri par similarit√© d√©croissante
        similarities.sort((a, b) => b.similarity - a.similarity);

        return similarities.slice(0, topK);
    }

    /**
     * G√©n√©ration de cl√© de cache bas√©e sur le hash du texte
     */
    generateCacheKey(text) {
        return natural.SoundEx.process(text) + '_' + text.length;
    }

    /**
     * Chargement du cache depuis le disque
     */
    async loadCache() {
        try {
            const cacheData = await fs.readFile(this.cacheFile, 'utf8');
            const cacheObject = JSON.parse(cacheData);
            
            for (const [key, value] of Object.entries(cacheObject)) {
                this.cache.set(key, value);
            }
            
            console.log(`üìÅ Cache d'embeddings charg√©: ${this.cache.size} entr√©es`);
        } catch (error) {
            console.log('üìÅ Nouveau cache d\'embeddings cr√©√©');
        }
    }

    /**
     * Sauvegarde du cache sur le disque
     */
    async saveCache() {
        try {
            const cacheObject = Object.fromEntries(this.cache);
            await fs.writeFile(this.cacheFile, JSON.stringify(cacheObject, null, 2));
            console.log(`üíæ Cache d'embeddings sauvegard√©: ${this.cache.size} entr√©es`);
        } catch (error) {
            console.error('Erreur sauvegarde cache:', error);
        }
    }

    /**
     * Nettoyage des ressources
     */
    async cleanup() {
        await this.saveCache();
        this.cache.clear();
        this.model = null;
        this.isInitialized = false;
    }

    /**
     * Statistiques du service
     */
    getStats() {
        return {
            isInitialized: this.isInitialized,
            modelName: this.modelName,
            cacheSize: this.cache.size,
            embeddingDimensions: 384
        };
    }
}

module.exports = EmbeddingService;
